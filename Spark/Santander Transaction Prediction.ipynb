{"cells":[{"cell_type":"markdown","source":["## Overview\n\nThis notebook demonstrates the working of a Data Science Project using Apache Spark. The dataset was uploaded to DBFS. [DBFS](https://docs.databricks.com/user-guide/dbfs-databricks-file-system.html) is a Databricks File System that allows you to store data for querying inside of Databricks. This notebook assumes that you have a file already inside of DBFS that you would like to read from.\n\nThis notebook is written in **Python** so the default cell type is Python. The dataset has been taken from [Kaggle](https://www.kaggle.com/c/santander-customer-transaction-prediction/overview). \n\nThe Project aims at predicting if a Santander customer will make a specific transaction in the future, irrespective of the amount of money transacted. The project will work through the data loading, exploratory analysis and feature selection, and finally building the model to correctly predict the behaviour."],"metadata":{}},{"cell_type":"markdown","source":["## Loading the dataset\n\nThe dataset was uploaded to the DBFS. The following code reads the dataset from the filesystem. The parameters are provided for infering the schema, header and delimiter for the csv. We'll continue to work with the default datatypes provided by spark and avoid converting it into a pandas dataframe because the execution of toPandas() will bring the data to the driver node. And since it is a considerable size of dataset, we'll avoid doing this and work with the default datasets that come with Spark."],"metadata":{}},{"cell_type":"code","source":["file_type = 'csv'\ntrain_file_location = \"/FileStore/tables/train.csv\"\ntrain = spark.read.format(file_type) \\\n  .option(\"inferSchema\", True) \\\n  .option(\"header\", True) \\\n  .option(\"sep\", ',') \\\n  .load(train_file_location)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":3},{"cell_type":"code","source":["test_file_location = \"/FileStore/tables/test.csv\"\ntest = spark.read.format(file_type) \\\n  .option(\"inferSchema\", True) \\\n  .option(\"header\", True) \\\n  .option(\"sep\", ',') \\\n  .load(test_file_location)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":4},{"cell_type":"markdown","source":["We'll now take a look at the dimensions of the training and test dataset."],"metadata":{}},{"cell_type":"code","source":["print('training dataset shape - ', train.count(), 'x', len(train.columns), '\\ntest dataset shape - ', test.count(), 'x', len(test.columns))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">training dataset shape -  200000 x 202 \ntest dataset shape -  200000 x 201\n</div>"]}}],"execution_count":6},{"cell_type":"code","source":["print('Columns for the training dataset are: ', train.columns)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Columns for the training dataset are:  [&#39;ID_code&#39;, &#39;target&#39;, &#39;var_0&#39;, &#39;var_1&#39;, &#39;var_2&#39;, &#39;var_3&#39;, &#39;var_4&#39;, &#39;var_5&#39;, &#39;var_6&#39;, &#39;var_7&#39;, &#39;var_8&#39;, &#39;var_9&#39;, &#39;var_10&#39;, &#39;var_11&#39;, &#39;var_12&#39;, &#39;var_13&#39;, &#39;var_14&#39;, &#39;var_15&#39;, &#39;var_16&#39;, &#39;var_17&#39;, &#39;var_18&#39;, &#39;var_19&#39;, &#39;var_20&#39;, &#39;var_21&#39;, &#39;var_22&#39;, &#39;var_23&#39;, &#39;var_24&#39;, &#39;var_25&#39;, &#39;var_26&#39;, &#39;var_27&#39;, &#39;var_28&#39;, &#39;var_29&#39;, &#39;var_30&#39;, &#39;var_31&#39;, &#39;var_32&#39;, &#39;var_33&#39;, &#39;var_34&#39;, &#39;var_35&#39;, &#39;var_36&#39;, &#39;var_37&#39;, &#39;var_38&#39;, &#39;var_39&#39;, &#39;var_40&#39;, &#39;var_41&#39;, &#39;var_42&#39;, &#39;var_43&#39;, &#39;var_44&#39;, &#39;var_45&#39;, &#39;var_46&#39;, &#39;var_47&#39;, &#39;var_48&#39;, &#39;var_49&#39;, &#39;var_50&#39;, &#39;var_51&#39;, &#39;var_52&#39;, &#39;var_53&#39;, &#39;var_54&#39;, &#39;var_55&#39;, &#39;var_56&#39;, &#39;var_57&#39;, &#39;var_58&#39;, &#39;var_59&#39;, &#39;var_60&#39;, &#39;var_61&#39;, &#39;var_62&#39;, &#39;var_63&#39;, &#39;var_64&#39;, &#39;var_65&#39;, &#39;var_66&#39;, &#39;var_67&#39;, &#39;var_68&#39;, &#39;var_69&#39;, &#39;var_70&#39;, &#39;var_71&#39;, &#39;var_72&#39;, &#39;var_73&#39;, &#39;var_74&#39;, &#39;var_75&#39;, &#39;var_76&#39;, &#39;var_77&#39;, &#39;var_78&#39;, &#39;var_79&#39;, &#39;var_80&#39;, &#39;var_81&#39;, &#39;var_82&#39;, &#39;var_83&#39;, &#39;var_84&#39;, &#39;var_85&#39;, &#39;var_86&#39;, &#39;var_87&#39;, &#39;var_88&#39;, &#39;var_89&#39;, &#39;var_90&#39;, &#39;var_91&#39;, &#39;var_92&#39;, &#39;var_93&#39;, &#39;var_94&#39;, &#39;var_95&#39;, &#39;var_96&#39;, &#39;var_97&#39;, &#39;var_98&#39;, &#39;var_99&#39;, &#39;var_100&#39;, &#39;var_101&#39;, &#39;var_102&#39;, &#39;var_103&#39;, &#39;var_104&#39;, &#39;var_105&#39;, &#39;var_106&#39;, &#39;var_107&#39;, &#39;var_108&#39;, &#39;var_109&#39;, &#39;var_110&#39;, &#39;var_111&#39;, &#39;var_112&#39;, &#39;var_113&#39;, &#39;var_114&#39;, &#39;var_115&#39;, &#39;var_116&#39;, &#39;var_117&#39;, &#39;var_118&#39;, &#39;var_119&#39;, &#39;var_120&#39;, &#39;var_121&#39;, &#39;var_122&#39;, &#39;var_123&#39;, &#39;var_124&#39;, &#39;var_125&#39;, &#39;var_126&#39;, &#39;var_127&#39;, &#39;var_128&#39;, &#39;var_129&#39;, &#39;var_130&#39;, &#39;var_131&#39;, &#39;var_132&#39;, &#39;var_133&#39;, &#39;var_134&#39;, &#39;var_135&#39;, &#39;var_136&#39;, &#39;var_137&#39;, &#39;var_138&#39;, &#39;var_139&#39;, &#39;var_140&#39;, &#39;var_141&#39;, &#39;var_142&#39;, &#39;var_143&#39;, &#39;var_144&#39;, &#39;var_145&#39;, &#39;var_146&#39;, &#39;var_147&#39;, &#39;var_148&#39;, &#39;var_149&#39;, &#39;var_150&#39;, &#39;var_151&#39;, &#39;var_152&#39;, &#39;var_153&#39;, &#39;var_154&#39;, &#39;var_155&#39;, &#39;var_156&#39;, &#39;var_157&#39;, &#39;var_158&#39;, &#39;var_159&#39;, &#39;var_160&#39;, &#39;var_161&#39;, &#39;var_162&#39;, &#39;var_163&#39;, &#39;var_164&#39;, &#39;var_165&#39;, &#39;var_166&#39;, &#39;var_167&#39;, &#39;var_168&#39;, &#39;var_169&#39;, &#39;var_170&#39;, &#39;var_171&#39;, &#39;var_172&#39;, &#39;var_173&#39;, &#39;var_174&#39;, &#39;var_175&#39;, &#39;var_176&#39;, &#39;var_177&#39;, &#39;var_178&#39;, &#39;var_179&#39;, &#39;var_180&#39;, &#39;var_181&#39;, &#39;var_182&#39;, &#39;var_183&#39;, &#39;var_184&#39;, &#39;var_185&#39;, &#39;var_186&#39;, &#39;var_187&#39;, &#39;var_188&#39;, &#39;var_189&#39;, &#39;var_190&#39;, &#39;var_191&#39;, &#39;var_192&#39;, &#39;var_193&#39;, &#39;var_194&#39;, &#39;var_195&#39;, &#39;var_196&#39;, &#39;var_197&#39;, &#39;var_198&#39;, &#39;var_199&#39;]\n</div>"]}}],"execution_count":7},{"cell_type":"code","source":["print('Columns for the test dataset are - ', test.columns)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Columns for the test dataset are -  [&#39;ID_code&#39;, &#39;var_0&#39;, &#39;var_1&#39;, &#39;var_2&#39;, &#39;var_3&#39;, &#39;var_4&#39;, &#39;var_5&#39;, &#39;var_6&#39;, &#39;var_7&#39;, &#39;var_8&#39;, &#39;var_9&#39;, &#39;var_10&#39;, &#39;var_11&#39;, &#39;var_12&#39;, &#39;var_13&#39;, &#39;var_14&#39;, &#39;var_15&#39;, &#39;var_16&#39;, &#39;var_17&#39;, &#39;var_18&#39;, &#39;var_19&#39;, &#39;var_20&#39;, &#39;var_21&#39;, &#39;var_22&#39;, &#39;var_23&#39;, &#39;var_24&#39;, &#39;var_25&#39;, &#39;var_26&#39;, &#39;var_27&#39;, &#39;var_28&#39;, &#39;var_29&#39;, &#39;var_30&#39;, &#39;var_31&#39;, &#39;var_32&#39;, &#39;var_33&#39;, &#39;var_34&#39;, &#39;var_35&#39;, &#39;var_36&#39;, &#39;var_37&#39;, &#39;var_38&#39;, &#39;var_39&#39;, &#39;var_40&#39;, &#39;var_41&#39;, &#39;var_42&#39;, &#39;var_43&#39;, &#39;var_44&#39;, &#39;var_45&#39;, &#39;var_46&#39;, &#39;var_47&#39;, &#39;var_48&#39;, &#39;var_49&#39;, &#39;var_50&#39;, &#39;var_51&#39;, &#39;var_52&#39;, &#39;var_53&#39;, &#39;var_54&#39;, &#39;var_55&#39;, &#39;var_56&#39;, &#39;var_57&#39;, &#39;var_58&#39;, &#39;var_59&#39;, &#39;var_60&#39;, &#39;var_61&#39;, &#39;var_62&#39;, &#39;var_63&#39;, &#39;var_64&#39;, &#39;var_65&#39;, &#39;var_66&#39;, &#39;var_67&#39;, &#39;var_68&#39;, &#39;var_69&#39;, &#39;var_70&#39;, &#39;var_71&#39;, &#39;var_72&#39;, &#39;var_73&#39;, &#39;var_74&#39;, &#39;var_75&#39;, &#39;var_76&#39;, &#39;var_77&#39;, &#39;var_78&#39;, &#39;var_79&#39;, &#39;var_80&#39;, &#39;var_81&#39;, &#39;var_82&#39;, &#39;var_83&#39;, &#39;var_84&#39;, &#39;var_85&#39;, &#39;var_86&#39;, &#39;var_87&#39;, &#39;var_88&#39;, &#39;var_89&#39;, &#39;var_90&#39;, &#39;var_91&#39;, &#39;var_92&#39;, &#39;var_93&#39;, &#39;var_94&#39;, &#39;var_95&#39;, &#39;var_96&#39;, &#39;var_97&#39;, &#39;var_98&#39;, &#39;var_99&#39;, &#39;var_100&#39;, &#39;var_101&#39;, &#39;var_102&#39;, &#39;var_103&#39;, &#39;var_104&#39;, &#39;var_105&#39;, &#39;var_106&#39;, &#39;var_107&#39;, &#39;var_108&#39;, &#39;var_109&#39;, &#39;var_110&#39;, &#39;var_111&#39;, &#39;var_112&#39;, &#39;var_113&#39;, &#39;var_114&#39;, &#39;var_115&#39;, &#39;var_116&#39;, &#39;var_117&#39;, &#39;var_118&#39;, &#39;var_119&#39;, &#39;var_120&#39;, &#39;var_121&#39;, &#39;var_122&#39;, &#39;var_123&#39;, &#39;var_124&#39;, &#39;var_125&#39;, &#39;var_126&#39;, &#39;var_127&#39;, &#39;var_128&#39;, &#39;var_129&#39;, &#39;var_130&#39;, &#39;var_131&#39;, &#39;var_132&#39;, &#39;var_133&#39;, &#39;var_134&#39;, &#39;var_135&#39;, &#39;var_136&#39;, &#39;var_137&#39;, &#39;var_138&#39;, &#39;var_139&#39;, &#39;var_140&#39;, &#39;var_141&#39;, &#39;var_142&#39;, &#39;var_143&#39;, &#39;var_144&#39;, &#39;var_145&#39;, &#39;var_146&#39;, &#39;var_147&#39;, &#39;var_148&#39;, &#39;var_149&#39;, &#39;var_150&#39;, &#39;var_151&#39;, &#39;var_152&#39;, &#39;var_153&#39;, &#39;var_154&#39;, &#39;var_155&#39;, &#39;var_156&#39;, &#39;var_157&#39;, &#39;var_158&#39;, &#39;var_159&#39;, &#39;var_160&#39;, &#39;var_161&#39;, &#39;var_162&#39;, &#39;var_163&#39;, &#39;var_164&#39;, &#39;var_165&#39;, &#39;var_166&#39;, &#39;var_167&#39;, &#39;var_168&#39;, &#39;var_169&#39;, &#39;var_170&#39;, &#39;var_171&#39;, &#39;var_172&#39;, &#39;var_173&#39;, &#39;var_174&#39;, &#39;var_175&#39;, &#39;var_176&#39;, &#39;var_177&#39;, &#39;var_178&#39;, &#39;var_179&#39;, &#39;var_180&#39;, &#39;var_181&#39;, &#39;var_182&#39;, &#39;var_183&#39;, &#39;var_184&#39;, &#39;var_185&#39;, &#39;var_186&#39;, &#39;var_187&#39;, &#39;var_188&#39;, &#39;var_189&#39;, &#39;var_190&#39;, &#39;var_191&#39;, &#39;var_192&#39;, &#39;var_193&#39;, &#39;var_194&#39;, &#39;var_195&#39;, &#39;var_196&#39;, &#39;var_197&#39;, &#39;var_198&#39;, &#39;var_199&#39;]\n</div>"]}}],"execution_count":8},{"cell_type":"markdown","source":["## Exploratory Analysis\n\nWe'll first start the exploratory analysis by visualizing the target variable and checking if the dataset is imbalanced or not. \n\nFor doing this, we'll select the target function from the train dataframe after converting it into an RDD. We'll convert it into an RDD because we need to use the map function. In order to avoid heavy shuffiling across the cluster beacuse of the collect used, we'll sample the dataframe by the ratio of 10% using stratified sampling. This will lower our shuffling to a great extent and give us an idea about the dataset."],"metadata":{}},{"cell_type":"code","source":["import seaborn as sns\ntarget_values = train.select('target').sampleBy('target', {0:0.1, 1:0.1}).rdd.map(lambda x: x.target).collect()\ndisplay(sns.countplot(target_values))"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["The image shown above is for sampled dataset. We'll identify the actual ratio of the imabalance below."],"metadata":{}},{"cell_type":"code","source":["train.select('target').groupby('target').count().toPandas()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>target</th>\n      <th>count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>20098</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>179902</td>\n    </tr>\n  </tbody>\n</table>\n</div>"]}}],"execution_count":12},{"cell_type":"markdown","source":["As we can see from the image, the dataset is highly imbalanced. Now we'll move on to do a check on the missing values in the dataset. We'll identify the number of missing values in the dataset. In order to do this we'll use the aggregated function. We'll build our own function null_check which takes the column name as parameter and returns the sum of null values back."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import isnan, isnull, col\ndef null_check(c):\n  vals = col(c).isNotNull() & isnan(c)\n  return sum(vals.cast(\"integer\")).alias(c)\n\ntrain.agg(*[null_check(c) for c in train.columns]).first()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[79]: Row(ID_code=0, target=0, var_0=0, var_1=0, var_2=0, var_3=0, var_4=0, var_5=0, var_6=0, var_7=0, var_8=0, var_9=0, var_10=0, var_11=0, var_12=0, var_13=0, var_14=0, var_15=0, var_16=0, var_17=0, var_18=0, var_19=0, var_20=0, var_21=0, var_22=0, var_23=0, var_24=0, var_25=0, var_26=0, var_27=0, var_28=0, var_29=0, var_30=0, var_31=0, var_32=0, var_33=0, var_34=0, var_35=0, var_36=0, var_37=0, var_38=0, var_39=0, var_40=0, var_41=0, var_42=0, var_43=0, var_44=0, var_45=0, var_46=0, var_47=0, var_48=0, var_49=0, var_50=0, var_51=0, var_52=0, var_53=0, var_54=0, var_55=0, var_56=0, var_57=0, var_58=0, var_59=0, var_60=0, var_61=0, var_62=0, var_63=0, var_64=0, var_65=0, var_66=0, var_67=0, var_68=0, var_69=0, var_70=0, var_71=0, var_72=0, var_73=0, var_74=0, var_75=0, var_76=0, var_77=0, var_78=0, var_79=0, var_80=0, var_81=0, var_82=0, var_83=0, var_84=0, var_85=0, var_86=0, var_87=0, var_88=0, var_89=0, var_90=0, var_91=0, var_92=0, var_93=0, var_94=0, var_95=0, var_96=0, var_97=0, var_98=0, var_99=0, var_100=0, var_101=0, var_102=0, var_103=0, var_104=0, var_105=0, var_106=0, var_107=0, var_108=0, var_109=0, var_110=0, var_111=0, var_112=0, var_113=0, var_114=0, var_115=0, var_116=0, var_117=0, var_118=0, var_119=0, var_120=0, var_121=0, var_122=0, var_123=0, var_124=0, var_125=0, var_126=0, var_127=0, var_128=0, var_129=0, var_130=0, var_131=0, var_132=0, var_133=0, var_134=0, var_135=0, var_136=0, var_137=0, var_138=0, var_139=0, var_140=0, var_141=0, var_142=0, var_143=0, var_144=0, var_145=0, var_146=0, var_147=0, var_148=0, var_149=0, var_150=0, var_151=0, var_152=0, var_153=0, var_154=0, var_155=0, var_156=0, var_157=0, var_158=0, var_159=0, var_160=0, var_161=0, var_162=0, var_163=0, var_164=0, var_165=0, var_166=0, var_167=0, var_168=0, var_169=0, var_170=0, var_171=0, var_172=0, var_173=0, var_174=0, var_175=0, var_176=0, var_177=0, var_178=0, var_179=0, var_180=0, var_181=0, var_182=0, var_183=0, var_184=0, var_185=0, var_186=0, var_187=0, var_188=0, var_189=0, var_190=0, var_191=0, var_192=0, var_193=0, var_194=0, var_195=0, var_196=0, var_197=0, var_198=0, var_199=0)</div>"]}}],"execution_count":14},{"cell_type":"markdown","source":["Careful examination shows that no column in the training dataset has null values. We'll do the same with testing dataset."],"metadata":{}},{"cell_type":"code","source":["test.agg(*[null_check(c) for c in test.columns]).first()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[80]: Row(ID_code=0, var_0=0, var_1=0, var_2=0, var_3=0, var_4=0, var_5=0, var_6=0, var_7=0, var_8=0, var_9=0, var_10=0, var_11=0, var_12=0, var_13=0, var_14=0, var_15=0, var_16=0, var_17=0, var_18=0, var_19=0, var_20=0, var_21=0, var_22=0, var_23=0, var_24=0, var_25=0, var_26=0, var_27=0, var_28=0, var_29=0, var_30=0, var_31=0, var_32=0, var_33=0, var_34=0, var_35=0, var_36=0, var_37=0, var_38=0, var_39=0, var_40=0, var_41=0, var_42=0, var_43=0, var_44=0, var_45=0, var_46=0, var_47=0, var_48=0, var_49=0, var_50=0, var_51=0, var_52=0, var_53=0, var_54=0, var_55=0, var_56=0, var_57=0, var_58=0, var_59=0, var_60=0, var_61=0, var_62=0, var_63=0, var_64=0, var_65=0, var_66=0, var_67=0, var_68=0, var_69=0, var_70=0, var_71=0, var_72=0, var_73=0, var_74=0, var_75=0, var_76=0, var_77=0, var_78=0, var_79=0, var_80=0, var_81=0, var_82=0, var_83=0, var_84=0, var_85=0, var_86=0, var_87=0, var_88=0, var_89=0, var_90=0, var_91=0, var_92=0, var_93=0, var_94=0, var_95=0, var_96=0, var_97=0, var_98=0, var_99=0, var_100=0, var_101=0, var_102=0, var_103=0, var_104=0, var_105=0, var_106=0, var_107=0, var_108=0, var_109=0, var_110=0, var_111=0, var_112=0, var_113=0, var_114=0, var_115=0, var_116=0, var_117=0, var_118=0, var_119=0, var_120=0, var_121=0, var_122=0, var_123=0, var_124=0, var_125=0, var_126=0, var_127=0, var_128=0, var_129=0, var_130=0, var_131=0, var_132=0, var_133=0, var_134=0, var_135=0, var_136=0, var_137=0, var_138=0, var_139=0, var_140=0, var_141=0, var_142=0, var_143=0, var_144=0, var_145=0, var_146=0, var_147=0, var_148=0, var_149=0, var_150=0, var_151=0, var_152=0, var_153=0, var_154=0, var_155=0, var_156=0, var_157=0, var_158=0, var_159=0, var_160=0, var_161=0, var_162=0, var_163=0, var_164=0, var_165=0, var_166=0, var_167=0, var_168=0, var_169=0, var_170=0, var_171=0, var_172=0, var_173=0, var_174=0, var_175=0, var_176=0, var_177=0, var_178=0, var_179=0, var_180=0, var_181=0, var_182=0, var_183=0, var_184=0, var_185=0, var_186=0, var_187=0, var_188=0, var_189=0, var_190=0, var_191=0, var_192=0, var_193=0, var_194=0, var_195=0, var_196=0, var_197=0, var_198=0, var_199=0)</div>"]}}],"execution_count":16},{"cell_type":"markdown","source":["Test dataset has no null values too. Now the next step going forward will be to examine the values in the dataset. Check if it has a skewness in it and any possible way to find out the collinearity in the columns."],"metadata":{}},{"cell_type":"code","source":["sampled_train = train.stat.sampleBy('target', {0:0.1, 1:0.1})"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":18},{"cell_type":"code","source":["import matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nplt.figure()\ncolumns = np.array([c for c in sampled_train.columns if c.startswith('var')]).reshape(40, 5)\nfig, axes = plt.subplots(40, 5, squeeze=True, figsize=(30, 100))\nfor i, a in enumerate(axes):\n  for j, s in enumerate(a):\n    s.title.set_text('Dist of: %s'%columns[i, j])\n    sns.distplot(sampled_train.select(columns[i, j]).rdd.flatMap(lambda x: x).collect(), ax=s)\n  \ndisplay(fig)"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":["We can see from the plots that the distribution of the columns is mostly normal, but a few of them are skewed. We'll check the skewness of the dataset and identify if something has to be done about it."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import skewness\nskew = list()\nfor c in train.columns:\n  skew.append(train.select(skewness(train[c])).rdd.flatMap(lambda x: x).collect())"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":21},{"cell_type":"code","source":["max(np.array(x[2:])), min(np.array(x[2:])), sum(np.array(x[2:]) > 0.5)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[32]: (array([-0.01399395]), array([-0.81181156]), array([0]))</div>"]}}],"execution_count":22},{"cell_type":"markdown","source":["The maximum skewness in the data is 0.26 which is relatively low. We'll check the kurtosis now"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import kurtosis\nkurt = list()\nfor c in train.columns:\n  kurt.append(train.select(kurtosis(train[c])).rdd.flatMap(lambda x: x).collect())"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":24},{"cell_type":"code","source":["max(np.array(x[2:])), min(np.array(x[2:])), sum(np.array(x[2:]) > 0.5)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[33]: (array([-0.01399395]), array([-0.81181156]), array([0]))</div>"]}}],"execution_count":25},{"cell_type":"markdown","source":["## Feature Selection\n\nWe'll now try to analyse if we can use PCA and analyse if we can use the feature reduction technique here. In order to do that, we'll have to first do the scaling of the dataset as we do not know what scale the columns are in. After that we'll see how much variablity is being explained by the components. We'll use the sampled dataframe for this. We'll create a new sampled dataframe such that the target variable is equally balanced."],"metadata":{}},{"cell_type":"code","source":["sampled_train = train.stat.sampleBy('target', {0:0.11, 1:1})"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":27},{"cell_type":"code","source":["sampled_train = sampled_train.drop('ID_code')"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":28},{"cell_type":"markdown","source":["Since the columns in our dataset have float values, we cannot directly apply the minmaxscaler to the dataset. We'll have to create a vector of the column and apply the minmaxscaler after that. The output will be original columns + scaled columns."],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.feature import MinMaxScaler\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import PCA\n\ncolumns = [c for c in sampled_train.columns if c != 'target']\n\nassembler = VectorAssembler(inputCols=columns, outputCol='features')\nscaler = MinMaxScaler(inputCol='features', outputCol='scaledfeatures')\n\npipeline = Pipeline(stages=[assembler, scaler])\nsampled_train = pipeline.fit(sampled_train).transform(sampled_train)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":30},{"cell_type":"code","source":["pca = PCA(k=100, inputCol='scaledfeatures', outputCol='pcafeatures')\nmodel = pca.fit(sampled_train)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":31},{"cell_type":"code","source":["sum(model.explainedVariance.values)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[7]: 0.5723450625434576</div>"]}}],"execution_count":32},{"cell_type":"markdown","source":["So even after taking 100 features from the PCA, we are still able to explain only 57% of the variability. So we'll drop the idea of taking PCA and focus on other methods to derive feature importance which will indeed help us in building model."],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.classification import RandomForestClassifier\n\nrf = RandomForestClassifier(labelCol=\"target\", featuresCol=\"scaledfeatures\",impurity='gini')\nrf_model = rf.fit(sampled_train)\nrf_model.featureImportances"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[8]: SparseVector(200, {0: 0.0491, 1: 0.0043, 2: 0.0402, 3: 0.0005, 5: 0.0003, 6: 0.0434, 7: 0.0003, 8: 0.0003, 9: 0.0002, 10: 0.0005, 11: 0.0007, 12: 0.0518, 13: 0.0174, 14: 0.0003, 15: 0.0009, 18: 0.0057, 20: 0.0004, 21: 0.0187, 22: 0.023, 24: 0.0002, 25: 0.0008, 26: 0.0564, 27: 0.0003, 32: 0.0003, 33: 0.0041, 36: 0.0031, 38: 0.0002, 40: 0.0101, 44: 0.0295, 45: 0.0003, 48: 0.0012, 50: 0.0003, 53: 0.0011, 54: 0.0007, 55: 0.0003, 61: 0.0004, 64: 0.0004, 66: 0.002, 67: 0.0053, 68: 0.0006, 69: 0.0006, 70: 0.0008, 71: 0.0002, 75: 0.0009, 76: 0.017, 78: 0.0255, 79: 0.0003, 80: 0.0385, 81: 0.11, 82: 0.0001, 85: 0.0028, 86: 0.0015, 87: 0.0006, 88: 0.0006, 90: 0.0014, 91: 0.0067, 92: 0.0049, 93: 0.0004, 94: 0.0004, 95: 0.0003, 97: 0.0004, 99: 0.0118, 101: 0.0005, 108: 0.0012, 109: 0.0013, 110: 0.0571, 111: 0.0006, 113: 0.0003, 115: 0.0074, 118: 0.0009, 119: 0.0007, 122: 0.002, 123: 0.0056, 125: 0.0005, 126: 0.0001, 127: 0.0011, 128: 0.0003, 129: 0.0003, 130: 0.002, 131: 0.001, 132: 0.0003, 133: 0.0132, 134: 0.0007, 138: 0.0005, 139: 0.0749, 143: 0.0003, 144: 0.0003, 145: 0.0012, 146: 0.0319, 147: 0.0006, 148: 0.0068, 149: 0.0022, 150: 0.0008, 152: 0.0002, 154: 0.0009, 156: 0.0006, 157: 0.0003, 158: 0.0003, 159: 0.0008, 160: 0.0007, 162: 0.0005, 165: 0.0206, 166: 0.0449, 167: 0.0002, 169: 0.0108, 170: 0.0052, 171: 0.0004, 172: 0.0006, 173: 0.0061, 174: 0.0185, 178: 0.0002, 179: 0.0082, 181: 0.0003, 183: 0.0002, 184: 0.0073, 187: 0.0003, 190: 0.0231, 191: 0.0119, 192: 0.003, 193: 0.0009, 194: 0.0005, 198: 0.0143, 199: 0.0003})</div>"]}}],"execution_count":34},{"cell_type":"code","source":["import pandas as pd\n\nfeature_imp_df = pd.DataFrame({'colname':columns, 'imp':rf_model.featureImportances}).sort_values('imp', ascending=False)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":35},{"cell_type":"code","source":["feature_imp_df.head(25).T"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>81</th>\n      <th>139</th>\n      <th>110</th>\n      <th>26</th>\n      <th>12</th>\n      <th>0</th>\n      <th>166</th>\n      <th>6</th>\n      <th>2</th>\n      <th>80</th>\n      <th>146</th>\n      <th>44</th>\n      <th>78</th>\n      <th>190</th>\n      <th>22</th>\n      <th>165</th>\n      <th>21</th>\n      <th>174</th>\n      <th>13</th>\n      <th>76</th>\n      <th>198</th>\n      <th>133</th>\n      <th>191</th>\n      <th>99</th>\n      <th>169</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>colname</th>\n      <td>var_81</td>\n      <td>var_139</td>\n      <td>var_110</td>\n      <td>var_26</td>\n      <td>var_12</td>\n      <td>var_0</td>\n      <td>var_166</td>\n      <td>var_6</td>\n      <td>var_2</td>\n      <td>var_80</td>\n      <td>var_146</td>\n      <td>var_44</td>\n      <td>var_78</td>\n      <td>var_190</td>\n      <td>var_22</td>\n      <td>var_165</td>\n      <td>var_21</td>\n      <td>var_174</td>\n      <td>var_13</td>\n      <td>var_76</td>\n      <td>var_198</td>\n      <td>var_133</td>\n      <td>var_191</td>\n      <td>var_99</td>\n      <td>var_169</td>\n    </tr>\n    <tr>\n      <th>imp</th>\n      <td>0.11002</td>\n      <td>0.0749469</td>\n      <td>0.0571374</td>\n      <td>0.0563614</td>\n      <td>0.0518402</td>\n      <td>0.0490933</td>\n      <td>0.0448938</td>\n      <td>0.0434187</td>\n      <td>0.0401545</td>\n      <td>0.0384796</td>\n      <td>0.0319045</td>\n      <td>0.0295492</td>\n      <td>0.0254687</td>\n      <td>0.0231182</td>\n      <td>0.022986</td>\n      <td>0.0205869</td>\n      <td>0.0186514</td>\n      <td>0.0185214</td>\n      <td>0.0174395</td>\n      <td>0.0169846</td>\n      <td>0.0143446</td>\n      <td>0.0131662</td>\n      <td>0.0119092</td>\n      <td>0.0117959</td>\n      <td>0.0108433</td>\n    </tr>\n  </tbody>\n</table>\n</div>"]}}],"execution_count":36},{"cell_type":"code","source":["feature_imp_df[feature_imp_df['imp'] == 0.0].shape"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"markdown","source":["So there are 58 columns in the dataset which contribute nothing towards the prediction of the model."],"metadata":{}},{"cell_type":"markdown","source":["Now we'll select the top 25 important features and will use those going forward in order to do hyper-parameter tuning and create a final model."],"metadata":{}},{"cell_type":"code","source":["imp_features = feature_imp_df['colname'][:25]\nind = feature_imp_df.index.values[:25]"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":40},{"cell_type":"markdown","source":["Here we'll use the VectorSlicer to get the values from the features which we have scaled."],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.feature import VectorSlicer\n\nslicer = VectorSlicer(inputCol='scaledfeatures', outputCol='impfeatures', indices=ind)\nsampled_train = slicer.transform(sampled_train)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":42},{"cell_type":"markdown","source":["## Splitting the data for training and validation.\n\nNow, we have the set of important features and their values in the column 'impfeatures'. We'll now proceed to building various models."],"metadata":{}},{"cell_type":"code","source":["X_train = train.sampleBy('target', {0:0.8, 1:0.8})\nX_test = train.subtract(X_train)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":44},{"cell_type":"code","source":["X_train.groupby('target').count().toPandas()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>target</th>\n      <th>count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>16134</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>143925</td>\n    </tr>\n  </tbody>\n</table>\n</div>"]}}],"execution_count":45},{"cell_type":"code","source":["X_test.groupby('target').count().toPandas()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>target</th>\n      <th>count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>3991</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>35948</td>\n    </tr>\n  </tbody>\n</table>\n</div>"]}}],"execution_count":46},{"cell_type":"markdown","source":["## Transforming the training and validation data \n\nWe'll create a function which takes the data in and transforms it into the format that we want using the pipeline we generated and slicer to get the values from only the important features."],"metadata":{}},{"cell_type":"code","source":["def transformer(train, pipeline, slicer):\n  df = spark.createDataFrame(sc.emptyRDD(), schema=train.schema)\n  for i in range(5):\n    df = df.union(train.sampleBy('target', {0:0.11, 1:1}))\n  df = pipeline.fit(df).transform(df)\n  df = slicer.transform(df)\n  return df"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":48},{"cell_type":"code","source":["def transformer_test(df, pipeline, slicer):\n  df = pipeline.fit(df).transform(df)\n  df = slicer.transform(df)\n  return df"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":49},{"cell_type":"markdown","source":["We'll transform the training data first to the required format."],"metadata":{}},{"cell_type":"code","source":["X_train_1 = transformer(X_train, pipeline, slicer)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":51},{"cell_type":"markdown","source":["We'll transform the test data to the required format."],"metadata":{}},{"cell_type":"code","source":["X_test = transformer_test(X_test, pipeline, slicer)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":53},{"cell_type":"markdown","source":["## Building  a model\n\nNow we have the data in the required format. We'll proceed to building a most suitable model which gives us more accuracy towards the validation dataset. We'll also do some hyper-parameter tuning to get the desired results."],"metadata":{}},{"cell_type":"markdown","source":["#### Logistic Regression"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml.tuning import ParamGridBuilder, CrossValidator\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\n\nevaluator = BinaryClassificationEvaluator(labelCol='target')\nlg = LogisticRegression(featuresCol='scaledfeatures', labelCol='target')\n\nlg_model = lg.fit(X_train_1)\ntraining_result_lr = lg_model.summary\npredictions_lr = lg_model.transform(X_test)\nprint(\"Area Under ROC for test dataset: \" + str(evaluator.evaluate(predictions_lr)))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Area Under ROC for test dataset: 0.8612999806721453\n</div>"]}}],"execution_count":56},{"cell_type":"markdown","source":["#### Random Forest Classifier"],"metadata":{}},{"cell_type":"code","source":["rf = RandomForestClassifier(labelCol=\"target\", featuresCol=\"scaledfeatures\", numTrees=50)\nrf_model = rf.fit(X_train_1)\n#training_result_rf = rf_model.summary\npredictions_rf = rf_model.transform(X_test)\nprint(\"Area Under ROC for test dataset: \" + str(evaluator.evaluate(predictions_rf)))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Area Under ROC for test dataset: 0.7420819745612224\n</div>"]}}],"execution_count":58},{"cell_type":"markdown","source":["#### GBTClassifier\n\nThe classfier gave best performance at stepsize 0.45. The performance deteiorated when used other hyper-parameters."],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.classification import GBTClassifier\n\ngbt = GBTClassifier(labelCol=\"target\", featuresCol=\"scaledfeatures\", stepSize=0.5)\ngbt_model = gbt.fit(X_train_1)\npredictions_gbt = gbt_model.transform(X_test)\nprint(\"Area Under ROC for test dataset: \" + str(evaluator.evaluate(predictions_gbt)))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Area Under ROC for test dataset: 0.8007288376610829\n</div>"]}}],"execution_count":60},{"cell_type":"markdown","source":["#### Multi Layer Perceptron Classifier"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.classification import MultilayerPerceptronClassifier\n\nlayers = [25, 20, 10, 2]\n\nmp = MultiLayerPerceptronClassifier(maxIters=10, layers=layers, blocksize=128)\nmp_model = mp.fit(X_train_1)\npredictions_mp = mp_model.transform(X_test)\nprint(\"Area Under ROC for test dataset: \" + str(evaluator.evaluate(predictions_mp)))"],"metadata":{},"outputs":[],"execution_count":62},{"cell_type":"code","source":["import matplotlib.pyplot as plt\nplt.figure(figsize=(3,3))\nroc = training_result.roc.toPandas()\nplt.plot(roc['FPR'],roc['TPR'])\nplt.ylabel('False Positive Rate')\nplt.xlabel('True Positive Rate')\nplt.title('ROC Curve')\ndisplay(plt.show())"],"metadata":{},"outputs":[],"execution_count":63},{"cell_type":"code","source":["predictions_lr.select('target', 'prediction').groupby(['target', 'prediction']).count().toPandas()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>target</th>\n      <th>prediction</th>\n      <th>count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0.0</td>\n      <td>722</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>0.0</td>\n      <td>117433</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>1.0</td>\n      <td>3387</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>1.0</td>\n      <td>46808</td>\n    </tr>\n  </tbody>\n</table>\n</div>"]}}],"execution_count":64}],"metadata":{"name":"2020-02-08 - DBFS Example","notebookId":1471931059479794},"nbformat":4,"nbformat_minor":0}
